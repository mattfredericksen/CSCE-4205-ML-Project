{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "feature_extraction.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPZoUIr+FT7oWOOCKjX5vb7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mattfredericksen/CSCE-4205-ML-Project/blob/main/feature_extraction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WoPyrrXci7Bd"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4rLWSn9cG4sf",
        "outputId": "fdcee46d-7dc4-4ae8-b3ad-1b3f90071c05",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import os\n",
        "import json\n",
        "import string\n",
        "from time import time\n",
        "from contextlib import suppress\n",
        "\n",
        "import gzip\n",
        "from urllib.request import urlopen\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fHnx1JGh93ls"
      },
      "source": [
        "# Progress Bar\n",
        "# https://colab.research.google.com/drive/1I2o3Ie34vJ3G4M6eE54-OyrmzJNBwhOp#scrollTo=EbF9oPhzOqZj\n",
        "\n",
        "from IPython.display import HTML, display\n",
        "\n",
        "def progress(value, max=100):\n",
        "    return HTML(f\"\"\"\n",
        "        <progress\n",
        "            value='{value}'\n",
        "            max='{max}',\n",
        "            style='width: 50%'\n",
        "        >\n",
        "            {value}\n",
        "        </progress>\n",
        "    \"\"\")"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-SKxbcgAHdnv"
      },
      "source": [
        "**Dataset links**\n",
        "- [Books (\\~30 million, too large!)](http://deepyeti.ucsd.edu/jianmo/amazon/categoryFilesSmall/Books_5.json.gz)  \n",
        "- [Clothing, Shoes, and Jewelry (\\~11 million)](http://deepyeti.ucsd.edu/jianmo/amazon/categoryFilesSmall/Clothing_Shoes_and_Jewelry_5.json.gz)    \n",
        "- [Electronics (\\~7 million)](http://deepyeti.ucsd.edu/jianmo/amazon/categoryFilesSmall/Electronics_5.json.gz  )  \n",
        "- [Home and Kitchen (\\~7 million)](http://deepyeti.ucsd.edu/jianmo/amazon/categoryFilesSmall/Home_and_Kitchen_5.json.gz)  \n",
        "- [Movies and TV (\\~3.5 million)](http://deepyeti.ucsd.edu/jianmo/amazon/categoryFilesSmall/Movies_and_TV_5.json.gz)  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a3TZa1CtEzo0",
        "outputId": "90f41603-0f76-4aae-c0e6-6fc7863ea4a5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!wget http://deepyeti.ucsd.edu/jianmo/amazon/categoryFilesSmall/Movies_and_TV_5.json.gz"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-11-10 16:17:57--  http://deepyeti.ucsd.edu/jianmo/amazon/categoryFilesSmall/Movies_and_TV_5.json.gz\n",
            "Resolving deepyeti.ucsd.edu (deepyeti.ucsd.edu)... 169.228.63.50\n",
            "Connecting to deepyeti.ucsd.edu (deepyeti.ucsd.edu)|169.228.63.50|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 791322468 (755M) [application/octet-stream]\n",
            "Saving to: ‘Movies_and_TV_5.json.gz’\n",
            "\n",
            "Movies_and_TV_5.jso 100%[===================>] 754.66M  21.5MB/s    in 35s     \n",
            "\n",
            "2020-11-10 16:18:33 (21.4 MB/s) - ‘Movies_and_TV_5.json.gz’ saved [791322468/791322468]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RmoClcgGYYrY"
      },
      "source": [
        "# Feature Engineering\n",
        "**List of features**  \n",
        "`reviewerID` - ID of the reviewer, e.g. A2SUAM1J3GNN3B  \n",
        "`asin` - ID of the product, e.g. 0000013714  \n",
        "`reviewerName` - name of the reviewer  \n",
        "`vote` - helpful votes of the review  \n",
        "`style` - a disctionary of the product metadata, e.g., \"Format\" is \"Hardcover\"  \n",
        "`reviewText` - text of the review  \n",
        "`overall` - rating of the product  \n",
        "`summary` - summary of the review  \n",
        "`unixReviewTime` - time of the review (unix time)  \n",
        "`reviewTime` - time of the review (raw)  \n",
        "`image` - images that users post after they have received the product  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "milsWnjMJlzB",
        "outputId": "394a8383-19ac-4b20-ee64-f4934e2af29f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "def drop_features(d):\n",
        "  kept_features = (\"overall\", \"reviewText\")\n",
        "  return {f: d[f] for f in kept_features}\n",
        "\n",
        "data = []\n",
        "\n",
        "with gzip.open(\"Movies_and_TV_5.json.gz\") as file:\n",
        "  for line in file:\n",
        "    with suppress(KeyError):\n",
        "      data.append(drop_features(json.loads(line.strip())))\n",
        "\n",
        "review_len = len(data)\n",
        "print(f'{review_len} reviews loaded.')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3408438 reviews loaded.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7AIC8juiM-9Y",
        "outputId": "e9c7367d-9091-414f-efed-f1742f707fae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 415
        }
      },
      "source": [
        "review_data = pd.DataFrame.from_dict(data)\n",
        "# del data\n",
        "review_data"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>overall</th>\n",
              "      <th>reviewText</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>5.0</td>\n",
              "      <td>So sorry I didn't purchase this years ago when...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>5.0</td>\n",
              "      <td>Believe me when I tell you that you will recei...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>5.0</td>\n",
              "      <td>I have seen X live many times, both in the ear...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>5.0</td>\n",
              "      <td>I was so excited for this!  Finally, a live co...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5.0</td>\n",
              "      <td>X is one of the best punk bands ever. I don't ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3408433</th>\n",
              "      <td>4.0</td>\n",
              "      <td>The singing parts are very good as expected fr...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3408434</th>\n",
              "      <td>5.0</td>\n",
              "      <td>This recording of the 2015 production by the M...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3408435</th>\n",
              "      <td>4.0</td>\n",
              "      <td>I do not wish to write a review about this rel...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3408436</th>\n",
              "      <td>5.0</td>\n",
              "      <td>It was a gift.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3408437</th>\n",
              "      <td>4.0</td>\n",
              "      <td>This Otello originates from the Salzburg Festi...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>3408438 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         overall                                         reviewText\n",
              "0            5.0  So sorry I didn't purchase this years ago when...\n",
              "1            5.0  Believe me when I tell you that you will recei...\n",
              "2            5.0  I have seen X live many times, both in the ear...\n",
              "3            5.0  I was so excited for this!  Finally, a live co...\n",
              "4            5.0  X is one of the best punk bands ever. I don't ...\n",
              "...          ...                                                ...\n",
              "3408433      4.0  The singing parts are very good as expected fr...\n",
              "3408434      5.0  This recording of the 2015 production by the M...\n",
              "3408435      4.0  I do not wish to write a review about this rel...\n",
              "3408436      5.0                                     It was a gift.\n",
              "3408437      4.0  This Otello originates from the Salzburg Festi...\n",
              "\n",
              "[3408438 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o4eGnUccPHTk"
      },
      "source": [
        "https://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction\n",
        "\n",
        "https://scikit-learn.org/stable/modules/feature_extraction.html#customizing-the-vectorizer-classes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n86vDEG7m0zL"
      },
      "source": [
        "class LemmaTokenizer:\n",
        "    def __init__(self, pb):\n",
        "        self.wnl = WordNetLemmatizer()\n",
        "        self.call_count = 0\n",
        "        self.pb = pb\n",
        "        # experimental\n",
        "        self.stopwords = set(stopwords.words('english')) | set(string.punctuation)\n",
        "\n",
        "    def __call__(self, doc):\n",
        "        self.call_count += 1\n",
        "        if self.call_count % 1024 == 0:\n",
        "            # temporarily cutting back review_len\n",
        "            self.pb.update(progress(self.call_count, review_len//10))\n",
        "        return tuple(self.wnl.lemmatize(t) for t in word_tokenize(doc) if t not in self.stopwords)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ILy3aqlnHbuT",
        "outputId": "8e0977a1-3a7e-40c8-d8fb-930513cc76fc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "vectorizer = TfidfVectorizer(stop_words='english')\n",
        "start = time()\n",
        "vectorizer.fit_transform(review_data['reviewText'][:review_len//10])\n",
        "print(f'execution time: {((time() - start) / 60):.1f} minutes')\n",
        "print(f'{len(vectorizer.get_feature_names())} features (unique words)')"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "execution time: 0.4 minutes\n",
            "179115 features (unique words)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ZpRvK3ccrH2",
        "outputId": "cd05d01d-dc7d-49f4-d6a2-bbe391af4c51",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "source": [
        "pb = display(progress(0, review_len//10), display_id=True)\n",
        "\n",
        "vectorizer = TfidfVectorizer(stop_words='english', tokenizer=LemmaTokenizer(pb))\n",
        "start = time()\n",
        "# temporarily cutting back review_len\n",
        "vectorizer.fit_transform(review_data['reviewText'][:review_len//10])\n",
        "print(f'execution time: {((time() - start) / 60):.1f} minutes')\n",
        "print(f'{len(vectorizer.get_feature_names())} features (unique words)')"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "        <progress\n",
              "            value='340992'\n",
              "            max='340843',\n",
              "            style='width: 50%'\n",
              "        >\n",
              "            340992\n",
              "        </progress>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['le', 'u'] not in stop_words.\n",
            "  'stop_words.' % sorted(inconsistent))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "execution time: 6.4 minutes\n",
            "270207 features (unique words)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1J4_LtRMEqHx",
        "outputId": "3024c21c-0ad2-49be-aaf1-79d3c224a574",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# we still need to fix tokens containing punctuation & other symbols\n",
        "print(vectorizer.get_feature_names()[:10])\n",
        "print(vectorizer.get_feature_names()[10000:10010])"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['\\x10\\x10\\x10\\x10\\x10\\x10\\x10\\x10\\x10\\x10\\x10\\x10\\x10\\x10\\x10\\x10\\x10\\x10\\x10\\x10\\x10\\x10\\x10\\x10\\x10\\x10\\x10\\x10\\x10\\x10\\x10\\x10\\x10\\x10\\x10\\x10\\x10\\x10\\x10\\x10\\x10\\x10\\x10\\x10\\x10\\x10\\x10\\x10\\x10\\x10\\x10\\x10\\x10\\x10\\x10\\x10\\x10\\x10\\x10\\x10\\x10\\x10\\x10\\x10\\x10\\x10\\x10\\x10\\x10\\x10\\x10\\x10\\x10\\x10\\x10\\x10\\x10\\x10\\x10\\x10\\x10\\x10\\x10\\x10\\x10\\x10\\x10\\x10\\x10\\x10\\x10\\x10\\x10\\x10\\x10\\x10\\x10\\x10\\x10\\x10\\x10disturbing', '\\x10a', '\\x10am', '\\x10roll', '\\x10this', '\\x10whispers', '\\x1b\\x1b\\x1b', '\\x1b\\x1b\\x1bjust', '\\x1bnot', \"''\"]\n",
            "['-order', '-ordinary', '-orgianal', '-origanal', '-original', '-oscar', '-ossesione-', '-oswald', '-other', '-others']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ixroIsHHiIay"
      },
      "source": [
        "# Training/Testing Split\n",
        "In the next cell, we randomly split the data into training and testing sets. At a later time, we may want switch to using [stratified K-fold cross validation](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedKFold.html#sklearn.model_selection.StratifiedKFold), which performs cross validation while ensuring an equal distribution of classes (star ratings)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RPQK9EiSchya"
      },
      "source": [
        "targets = review_data[['overall']]\n",
        "features = review_data[['reviewText']]\n",
        "train_features, test_features, train_targets, test_targets = train_test_split(features, targets, test_size=0.2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DPinOM4QV7rT"
      },
      "source": [
        "https://scikit-learn.org/stable/modules/naive_bayes.html#multinomial-naive-bayes"
      ]
    }
  ]
}