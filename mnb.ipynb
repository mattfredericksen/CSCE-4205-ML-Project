{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Copy of feature_extraction.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mattfredericksen/CSCE-4205-ML-Project/blob/main/mnb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WoPyrrXci7Bd"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4rLWSn9cG4sf"
      },
      "source": [
        "import os\n",
        "import json\n",
        "import re\n",
        "from time import time\n",
        "from contextlib import suppress\n",
        "import gc\n",
        "\n",
        "import gzip\n",
        "import pickle\n",
        "import gdown\n",
        "from urllib.request import urlopen\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.model_selection import RepeatedKFold\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import cross_validate\n",
        "\n",
        "from scipy import stats\n",
        "\n",
        "import spacy\n",
        "spacy.prefer_gpu()\n",
        "nlp = spacy.load(\"en_core_web_sm\", disable=['parser', 'ner'])"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fHnx1JGh93ls"
      },
      "source": [
        "# Progress Bar\n",
        "# https://colab.research.google.com/drive/1I2o3Ie34vJ3G4M6eE54-OyrmzJNBwhOp#scrollTo=EbF9oPhzOqZj\n",
        "\n",
        "from IPython.display import HTML, display\n",
        "\n",
        "def progress(value, max=100):\n",
        "    return HTML(f\"\"\"\n",
        "        <progress\n",
        "            value='{value}'\n",
        "            max='{max}',\n",
        "            style='width: 50%'\n",
        "        >\n",
        "            {value}\n",
        "        </progress>\n",
        "    \"\"\")"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RmoClcgGYYrY"
      },
      "source": [
        "# Feature Engineering\n",
        "\n",
        "**Dataset links**\n",
        "- [Books (\\~30 million, too large!)](http://deepyeti.ucsd.edu/jianmo/amazon/categoryFilesSmall/Books_5.json.gz)  \n",
        "- [Clothing, Shoes, and Jewelry (\\~11 million)](http://deepyeti.ucsd.edu/jianmo/amazon/categoryFilesSmall/Clothing_Shoes_and_Jewelry_5.json.gz)    \n",
        "- [Electronics (\\~7 million)](http://deepyeti.ucsd.edu/jianmo/amazon/categoryFilesSmall/Electronics_5.json.gz  )  \n",
        "- [Home and Kitchen (\\~7 million)](http://deepyeti.ucsd.edu/jianmo/amazon/categoryFilesSmall/Home_and_Kitchen_5.json.gz)  \n",
        "- [Movies and TV (\\~3.5 million)](http://deepyeti.ucsd.edu/jianmo/amazon/categoryFilesSmall/Movies_and_TV_5.json.gz)  \n",
        "\n",
        "**List of features**  \n",
        "`reviewerID` - ID of the reviewer, e.g. A2SUAM1J3GNN3B  \n",
        "`asin` - ID of the product, e.g. 0000013714  \n",
        "`reviewerName` - name of the reviewer  \n",
        "`vote` - helpful votes of the review  \n",
        "`style` - a disctionary of the product metadata, e.g., \"Format\" is \"Hardcover\"  \n",
        "`reviewText` - text of the review  \n",
        "`overall` - rating of the product  \n",
        "`summary` - summary of the review  \n",
        "`unixReviewTime` - time of the review (unix time)  \n",
        "`reviewTime` - time of the review (raw)  \n",
        "`image` - images that users post after they have received the product  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uWqnOVWdG8s2"
      },
      "source": [
        "### Download All Data\n",
        "This section shows how we originally retrieved the data. We recommend using the later cells to load our pre-processed data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a3TZa1CtEzo0"
      },
      "source": [
        "!wget http://deepyeti.ucsd.edu/jianmo/amazon/categoryFilesSmall/Movies_and_TV_5.json.gz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "milsWnjMJlzB"
      },
      "source": [
        "def drop_features(d):\n",
        "  kept_features = (\"overall\", \"reviewText\")\n",
        "  return {f: d[f] for f in kept_features}\n",
        "\n",
        "data = []\n",
        "\n",
        "with gzip.open(\"Movies_and_TV_5.json.gz\") as file:\n",
        "  for line in file:\n",
        "    with suppress(KeyError):\n",
        "      data.append(drop_features(json.loads(line.strip())))\n",
        "\n",
        "print(f'{len(data)} reviews loaded.')\n",
        "data = pd.DataFrame.from_dict(data)\n",
        "data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fMVuY61xHFPJ"
      },
      "source": [
        "#### Download Partial Data\n",
        "We used this smaller dataset when initially figuring out how to use Lemmatizing and Vectorizing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nLgfcAimF_7r"
      },
      "source": [
        "# Faster, partial data download\n",
        "\n",
        "# !wget https://drive.google.com/uc?export=download&id=1_sieKFN89ry-owWOUWzWwmgbjeJb1xYu\n",
        "!wget -O cut_data.json https://www.dropbox.com/s/ostscnaq8eukdwy/cutData.json?dl=1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ctGYXK19HNbs"
      },
      "source": [
        "with open(\"cut_data.json\") as file:\n",
        "    data = pd.read_json(file)\n",
        "\n",
        "print(f'{len(data)} reviews loaded.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2elWyqvWHWFP"
      },
      "source": [
        "## Text Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G5pz7iyn_yyg"
      },
      "source": [
        "### From Raw Documents\n",
        "This section shows how we originally processed the data. We recommend using the later cells to load our pre-processed data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o4eGnUccPHTk"
      },
      "source": [
        "https://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction\n",
        "\n",
        "https://scikit-learn.org/stable/modules/feature_extraction.html#customizing-the-vectorizer-classes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n86vDEG7m0zL"
      },
      "source": [
        "class LemmaTokenizer:\n",
        "    def __init__(self, data):\n",
        "        # used to trim sequences of the \n",
        "        # same character to no more than 2\n",
        "        self.shorten_re = re.compile(r'(.)\\1{2,}')\n",
        "        # remove non-alphabetical characters\n",
        "        self.words_re = re.compile(r'[^a-z]')\n",
        "\n",
        "        self.call_count = 0\n",
        "        self.pb_len = len(data)\n",
        "        self.pb = display(progress(0, self.pb_len), display_id=True)\n",
        "\n",
        "    def __call__(self, doc):\n",
        "        self.call_count += 1\n",
        "        if self.call_count % 1024 == 0:\n",
        "            self.pb.update(progress(self.call_count % self.pb_len, self.pb_len))\n",
        "        # apply conver to lower case and apply our regex patterns\n",
        "        doc = self.shorten_re.sub(r'\\1\\1', self.words_re.sub(' ', doc.lower()))\n",
        "        # remove stop words and apply lemmatization\n",
        "        return tuple(t.lemma_ for t in nlp(doc) if not t.is_stop)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ckCeuTajrnyZ"
      },
      "source": [
        "tokenizer = LemmaTokenizer(data)\n",
        "start = time()\n",
        "data['reviewText'] = data['reviewText'].apply(tokenizer)\n",
        "print(f'execution time: {((time() - start) / 60):.1f} minutes')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OiBTQHopAM0P"
      },
      "source": [
        "### From Pre-Lemmatized Documents\n",
        "Use this section to quickly load the results of previous sections."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q500Rj_PpS-K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2b930c63-be47-41d9-cf7c-3fd1dc6ceb33"
      },
      "source": [
        "!wget -O processed.pkl https://www.dropbox.com/s/t3nbo9hyteir0s6/all_lemmas.pkl?dl=1"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-12-06 20:33:53--  https://www.dropbox.com/s/t3nbo9hyteir0s6/all_lemmas.pkl?dl=1\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.5.1, 2620:100:601d:1::a27d:501\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.5.1|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: /s/dl/t3nbo9hyteir0s6/all_lemmas.pkl [following]\n",
            "--2020-12-06 20:33:53--  https://www.dropbox.com/s/dl/t3nbo9hyteir0s6/all_lemmas.pkl\n",
            "Reusing existing connection to www.dropbox.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://uc36b776da27187c178aac58e465.dl.dropboxusercontent.com/cd/0/get/BEkufqvEblm7bvNXdE14Xea1hmyknfhIQATSfTL7ybmcQkslW0QLrjq2jMCSGlR0MGBObpUoRs471BKiUiE6BaTtG3SEzzmWaMZz5-uwn1iNwnHAAjTP15JAojX6cySXgVg/file?dl=1# [following]\n",
            "--2020-12-06 20:33:54--  https://uc36b776da27187c178aac58e465.dl.dropboxusercontent.com/cd/0/get/BEkufqvEblm7bvNXdE14Xea1hmyknfhIQATSfTL7ybmcQkslW0QLrjq2jMCSGlR0MGBObpUoRs471BKiUiE6BaTtG3SEzzmWaMZz5-uwn1iNwnHAAjTP15JAojX6cySXgVg/file?dl=1\n",
            "Resolving uc36b776da27187c178aac58e465.dl.dropboxusercontent.com (uc36b776da27187c178aac58e465.dl.dropboxusercontent.com)... 162.125.5.15, 2620:100:601d:15::a27d:50f\n",
            "Connecting to uc36b776da27187c178aac58e465.dl.dropboxusercontent.com (uc36b776da27187c178aac58e465.dl.dropboxusercontent.com)|162.125.5.15|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 976133114 (931M) [application/binary]\n",
            "Saving to: ‘processed.pkl’\n",
            "\n",
            "processed.pkl       100%[===================>] 930.91M  59.5MB/s    in 17s     \n",
            "\n",
            "2020-12-06 20:34:12 (54.6 MB/s) - ‘processed.pkl’ saved [976133114/976133114]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OLfrD82Gqctc"
      },
      "source": [
        "We convert the star ratings to a binary classification here because the low classes (1-3) occur with significantly less frequency than the high classes (4-5)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zrbo2G2Pp8MK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 433
        },
        "outputId": "2ca8caaf-b620-4130-961f-924e3657154b"
      },
      "source": [
        "data = pd.read_pickle(\"processed.pkl\")\n",
        "\n",
        "data['overall'] = data['overall'].apply(lambda x: 1 if x > 3 else 0)\n",
        "\n",
        "print(f'{len(data)} lemmatized reviews loaded.')\n",
        "data"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3408438 lemmatized reviews loaded.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>overall</th>\n",
              "      <th>reviewText</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>sorry didn t purchase year ago come   good ent...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>believe tell receive blessing watch video cath...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>see x live time   early day recent reunion sho...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>excited   finally   live concert video x   ve ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>x good punk band   don t like call punk band  ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3408433</th>\n",
              "      <td>1</td>\n",
              "      <td>singing part good expect sonya yoncheva zeljik...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3408434</th>\n",
              "      <td>1</td>\n",
              "      <td>recording   production metropolitan opera verd...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3408435</th>\n",
              "      <td>1</td>\n",
              "      <td>wish write review release   like point voice m...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3408436</th>\n",
              "      <td>1</td>\n",
              "      <td>gift</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3408437</th>\n",
              "      <td>1</td>\n",
              "      <td>otello originate salzburg festival   apparent ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>3408438 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         overall                                         reviewText\n",
              "0              1  sorry didn t purchase year ago come   good ent...\n",
              "1              1  believe tell receive blessing watch video cath...\n",
              "2              1  see x live time   early day recent reunion sho...\n",
              "3              1  excited   finally   live concert video x   ve ...\n",
              "4              1  x good punk band   don t like call punk band  ...\n",
              "...          ...                                                ...\n",
              "3408433        1  singing part good expect sonya yoncheva zeljik...\n",
              "3408434        1  recording   production metropolitan opera verd...\n",
              "3408435        1  wish write review release   like point voice m...\n",
              "3408436        1                                               gift\n",
              "3408437        1  otello originate salzburg festival   apparent ...\n",
              "\n",
              "[3408438 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ixroIsHHiIay"
      },
      "source": [
        "# Training/Testing Split\n",
        "In the next cell, we randomly split the data into training and testing sets. We will use training set with 5x2 cross validation to tune hyperparameters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RPQK9EiSchya",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "af3f3f78-ff70-445c-e6b0-d099585d0c3a"
      },
      "source": [
        "targets = data['overall']\n",
        "features = data['reviewText']\n",
        "train_features, test_features, train_targets, test_targets = (\n",
        "    train_test_split(features, targets, test_size=0.2, stratify=targets, random_state=123456)\n",
        ")\n",
        "# train_features, tune_features, train_targets, tune_targets = (\n",
        "#     train_test_split(train_features, train_targets, test_size=0.2, stratify=train_targets, random_state=654321)\n",
        "# )\n",
        "\n",
        "# stratification maintains an even class distribution\n",
        "print([\n",
        "       train_targets.sum() / len(train_targets),\n",
        "       test_targets.sum() / len(test_targets)\n",
        "])"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.7901795177409003, 0.7901796716386382]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s9NkxousAcBr"
      },
      "source": [
        "# Create Count and TF-IDF Vectors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Oj2C6YreTHt"
      },
      "source": [
        "### Vectorizers from lemmatized samples\n",
        "We recommend using the pickled vectorizers in the next section.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vFvSHYiVrL4i"
      },
      "source": [
        "vectorizer = TfidfVectorizer(min_df=1e-6, ngram_range=(1, 2))\n",
        "\n",
        "start = time()\n",
        "train_matrix = vectorizer.fit_transform(train_features)\n",
        "\n",
        "print(f'execution time: {((time() - start) / 60):.1f} minutes')\n",
        "print(f'{len(vectorizer.get_feature_names())} features (unique words)')\n",
        "train_matrix"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PDvOxlS74qP8"
      },
      "source": [
        "# with open(\"vectorizer.pkl\", 'wb') as file:\n",
        "#     pickle.dump(vectorizer, file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1J4_LtRMEqHx"
      },
      "source": [
        "print(vectorizer.get_feature_names()[:50])\n",
        "print(vectorizer.get_feature_names()[5000:5050])\n",
        "print(vectorizer.get_feature_names()[-50:])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PdRT20C8eXKv"
      },
      "source": [
        "### From pickled vectorizers\n",
        "This function can be called to download and unpickle vectorizers that were previously created from our training data. This process took about an hour, so using these saves a lot of time.\n",
        "\n",
        "The names represent the class/parameters used to create the vectorizers:\n",
        "- \"min_df{n}\" means the parameter \"min_df\" was set to \"1e-{n}\".\n",
        "- \"ngrams\" means the parameter \"ngram_range\" was set to \"(1, 2)\", meaning that uni-grams and bi-grams were processed. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e2eN86yOed7F"
      },
      "source": [
        "def load_vectorizer(name):\n",
        "    url_template = 'https://drive.google.com/uc?id={}'\n",
        "    vectorizer_names = {\n",
        "        'CountVectorizer-min_df5-ngrams': '107O5GW0aIqla6pYJrE1YfGiaANRNypXg',\n",
        "        'CountVectorizer-min_df5': '1K9Sd-nme_N3iHzKYRPv46UsYSMJf3-fx',\n",
        "        'CountVectorizer-min_df6-ngrams': '101TIMpeg3O-31q1zvePZYhUf6pTwEPzM',\n",
        "        'CountVectorizer-min_df6': '10-pwX_w74rguvLGqRMAjHU-KA1XAwGvh',\n",
        "        'CountVectorizer-min_df7': '10HPukhM8ZF91BeU5ZgtK2QIx9oK3gU6h',\n",
        "        'TfidfVectorizer-min_df5-ngrams': '1-jkMtmwz6HXms3Y26bUWv2mt3OqHJqLc',\n",
        "        'TfidfVectorizer-min_df5': '1-sWbjPJup-v31smQZP9IczDtIaH0prS2',\n",
        "        'TfidfVectorizer-min_df6-ngrams': '10X7sWhMVPDhfdKrvqQerRq03sknBscLA',\n",
        "        'TfidfVectorizer-min_df6': '103qmnyHPlRfJ2E6ZMevRCFFhTSP-sfee',\n",
        "        'TfidfVectorizer-min_df7': '1-uDkBESf7ySa2ixNtA4cDtocjeMUlRvO',\n",
        "    }\n",
        "    if name not in vectorizer_names:\n",
        "        raise ValueError()\n",
        "    if not os.path.exists(f'{name}.pkl'):\n",
        "        gdown.download(url_template.format(vectorizer_names[name]), f'{name}.pkl', False)\n",
        "    with open(f'{name}.pkl', 'rb') as file:\n",
        "        return pickle.load(file)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YitfFdKBrYX7"
      },
      "source": [
        "## Create the vectors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vNFPgMXhrqJn"
      },
      "source": [
        "vectorizer = load_vectorizer('CountVectorizer-min_df6')\n",
        "train_vectors = vectorizer.transform(train_features)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "05Y51jTCHXjk"
      },
      "source": [
        "# Create the Classifiers\n",
        "\n",
        "Our goal is to perform a valid statistical test comparing models from two algorithms:\n",
        "- Multinomial Naive Bayes with uni-gram CountVectorizer\n",
        "- Multinomial Naive Bayes with uni-gram + bi-gram TfidfVectorizer.\n",
        "\n",
        "Before performing these tests, will will perform 5x2 cross validation using our training data to tune a few hyperparameters:\n",
        "1. The \"min_df\" Vectorizer parameter, which sets a minimum threshold for how frequently a term must occur to be retained by the vectorizer.\n",
        "2. The \"alpha\" MultinomialNB parameter, which determines how much smoothing is applied (smoothing is used essentially so that no errors occur when previously unseen tokens are encountered)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OyySRDg8G2qm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1a578d65-20dd-4009-a3ce-fc61e3b0eae7"
      },
      "source": [
        "rkf = RepeatedKFold(n_splits=2, n_repeats=5, random_state=654321)\n",
        "scoring = ['f1', 'accuracy']\n",
        "\n",
        "best_f1 = 0\n",
        "best_count_vec_name = ''\n",
        "best_count_alpha = -1\n",
        "\n",
        "for min_df in (5, 6, 7):\n",
        "    vec_name = f'CountVectorizer-min_df{min_df}'\n",
        "    print(f'loading vectorizer: {vec_name}')\n",
        "    vectorizer = load_vectorizer(vec_name)\n",
        "    print('creating vectors...')\n",
        "    train_vectors = vectorizer.transform(train_features)\n",
        "    for alpha in (1, 1e-3, 1e-10):\n",
        "        print(f'\\talpha: {alpha}')\n",
        "        clf = MultinomialNB(alpha=alpha)\n",
        "        scores = cross_validate(clf, train_vectors, train_targets, scoring=scoring, cv=rkf)\n",
        "        for metric in scoring:\n",
        "            print(f'\\t\\t{metric}:', scores[f'test_{metric}'].mean())\n",
        "        \n",
        "        if scores['test_f1'].mean() > best_f1:\n",
        "            best_f1 = scores['test_f1'].mean()\n",
        "            best_count_vec_name = vec_name\n",
        "            best_count_alpha = alpha\n",
        "\n",
        "print('best count vectorizer:', best_count_vec_name)\n",
        "print('best MNBayes alpha:', best_count_alpha)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loading vectorizer: CountVectorizer-min_df5\n",
            "creating vectors...\n",
            "\talpha: 1\n",
            "\t\tf1: 0.9080594183567499\n",
            "\t\taccuracy: 0.8519818098468873\n",
            "\talpha: 0.001\n",
            "\t\tf1: 0.9080503971490369\n",
            "\t\taccuracy: 0.8519722013385899\n",
            "\talpha: 1e-10\n",
            "\t\tf1: 0.9080366751714909\n",
            "\t\taccuracy: 0.8519319336206106\n",
            "loading vectorizer: CountVectorizer-min_df6\n",
            "creating vectors...\n",
            "\talpha: 1\n",
            "\t\tf1: 0.9082408459375919\n",
            "\t\taccuracy: 0.8521923902081232\n",
            "\talpha: 0.001\n",
            "\t\tf1: 0.9073507275853275\n",
            "\t\taccuracy: 0.8506047125699092\n",
            "\talpha: 1e-10\n",
            "\t\tf1: 0.9062130702029669\n",
            "\t\taccuracy: 0.8483555881543963\n",
            "loading vectorizer: CountVectorizer-min_df7\n",
            "creating vectors...\n",
            "\talpha: 1\n",
            "\t\tf1: 0.9100363120638251\n",
            "\t\taccuracy: 0.8541965343357477\n",
            "\talpha: 0.001\n",
            "\t\tf1: 0.9063157058244601\n",
            "\t\taccuracy: 0.8492630420830659\n",
            "\talpha: 1e-10\n",
            "\t\tf1: 0.9050539750999536\n",
            "\t\taccuracy: 0.8467457596039243\n",
            "best count vectorizer: CountVectorizer-min_df7\n",
            "best MNBayes alpha: 1e-10\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S54XEskt1se8",
        "outputId": "8aa58b6a-ed23-4285-8f36-3c5a5c9f3d8e"
      },
      "source": [
        "rkf = RepeatedKFold(n_splits=2, n_repeats=5, random_state=654321)\n",
        "scoring = ['f1', 'accuracy']\n",
        "\n",
        "best_f1 = 0\n",
        "best_tfidf_vec_name = ''\n",
        "best_tfidf_alpha = -1\n",
        "\n",
        "for min_df in (5, 6):\n",
        "    vec_name = f'TfidfVectorizer-min_df{min_df}-ngrams'\n",
        "    print(f'loading vectorizer: {vec_name}')\n",
        "    vectorizer = load_vectorizer(vec_name)\n",
        "    print('creating vectors...')\n",
        "    train_vectors = vectorizer.transform(train_features)\n",
        "    for alpha in (1, 1e-3, 1e-10):\n",
        "        print(f'\\talpha: {alpha}')\n",
        "        clf = MultinomialNB(alpha=alpha)\n",
        "        scores = cross_validate(clf, train_vectors, train_targets, scoring=scoring, cv=rkf)\n",
        "        for metric in scoring:\n",
        "            print(f'\\t\\t{metric}:', scores[f'test_{metric}'].mean())\n",
        "        \n",
        "        if scores['test_f1'].mean() > best_f1:\n",
        "            best_f1 = scores['test_f1'].mean()\n",
        "            best_tfidf_vec_name = vec_name\n",
        "            best_tfidf_alpha = alpha\n",
        "\n",
        "print('best TF-IDF vectorizer:', best_tfidf_vec_name)\n",
        "print('best MNBayes alpha:', best_tfidf_alpha)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loading vectorizer: TfidfVectorizer-min_df5-ngrams\n",
            "creating vectors...\n",
            "\talpha: 1\n",
            "\t\tf1: 0.9213327470672825\n",
            "\t\taccuracy: 0.8692978087466765\n",
            "\talpha: 0.001\n",
            "\t\tf1: 0.9231617509175358\n",
            "\t\taccuracy: 0.8739723847070688\n",
            "\talpha: 1e-10\n",
            "\t\tf1: 0.9228786917782139\n",
            "\t\taccuracy: 0.8732486659943154\n",
            "loading vectorizer: TfidfVectorizer-min_df6-ngrams\n",
            "creating vectors...\n",
            "\talpha: 1\n",
            "\t\tf1: 0.8934064084306508\n",
            "\t\taccuracy: 0.8116769047400751\n",
            "\talpha: 0.001\n",
            "\t\tf1: 0.9184109650737675\n",
            "\t\taccuracy: 0.8650039790959934\n",
            "\talpha: 1e-10\n",
            "\t\tf1: 0.9054866801979988\n",
            "\t\taccuracy: 0.8413683689373797\n",
            "best TF-IDF vectorizer: TfidfVectorizer-min_df5-ngrams\n",
            "best MNBayes alpha: 0.001\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sBx27Tpx2Io-"
      },
      "source": [
        "# Statistical Tests"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "554JgU9b-i1E"
      },
      "source": [
        "rkf = RepeatedKFold(n_splits=10 ,n_repeats=1, random_state=123321)\n",
        "\n",
        "clf = MultinomialNB(alpha=best_count_alpha)\n",
        "test_vectors = load_vectorizer(best_count_vec_name).transform(test_features)\n",
        "count_scores = cross_validate(clf, test_vectors, test_targets, scoring='f1', cv=rkf)\n",
        "\n",
        "clf = MultinomialNB(alpha=best_tfidf_alpha)\n",
        "test_vectors = load_vectorizer(best_tfidf_vec_name).transform(test_features)\n",
        "tfidf_scores = cross_validate(clf, test_vectors, test_targets, scoring='f1', cv=rkf)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kDqGCNrsERxn",
        "outputId": "f0b5595e-bd44-4ddb-bcad-75ff0c8b8ffb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "stats.ttest_ind(count_scores['test_score'], tfidf_scores['test_score'])"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Ttest_indResult(statistic=-26.198402658751434, pvalue=8.722671215651108e-16)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    }
  ]
}